{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add hidden layers to your network to uncover complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode ini adalah contoh implementasi jaringan saraf tiruan (neural network) yang lebih kompleks menggunakan TensorFlow.\n",
    "\n",
    "Pertama, kita mengimpor TensorFlow dan modul layers dari Keras, yang akan digunakan untuk membangun model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robit\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # the hidden ReLU layers\n",
    "    layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
    "    layers.Dense(units=3, activation='relu'),\n",
    "    # the linear output layer \n",
    "    layers.Dense(units=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* layers.Dense: Lapisan-lapisan ini adalah lapisan Dense (lapisan yang sepenuhnya terhubung), yang akan menerima input dari lapisan sebelumnya dan mentransformasinya menggunakan fungsi aktivasi tertentu.\n",
    "* units: Menentukan jumlah unit (neuron) dalam lapisan Dense.\n",
    "* activation: Menentukan fungsi aktivasi yang akan digunakan. Di sini, kita menggunakan ReLU (Rectified Linear Unit) untuk lapisan-lapisan tersembunyi dan tidak menentukan fungsi aktivasi untuk lapisan keluaran.\n",
    "* input_shape: Menentukan bentuk input yang akan diterima oleh model. Dalam kasus ini, input memiliki panjang 2, karena kita memiliki dua fitur.\n",
    "\n",
    "Dengan demikian, model ini terdiri dari dua lapisan tersembunyi dengan fungsi aktivasi ReLU dan satu lapisan keluaran yang menggunakan fungsi identitas (linear). Lapisan-lapisan ini akan digunakan untuk mempelajari pola-pola dalam data dan menghasilkan prediksi yang akurat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
